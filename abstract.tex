% -*-mode: latex; coding: utf-8-unix -*-
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
%
% user-modifiable options
%
\usepackage{leading} \leading{6mm} % extra space between lines

% \usepackage[francais,UKenglish]{babel}    % last one is default language
\usepackage[a4paper]{geometry}
% \usepackage{lmodern} %% Vector fonts
%
% Font and appearance: do not change
%
% \usepackage{aeguill} %% French guillemets with T1
\usepackage{amsfonts,amsmath,amssymb} %% Additional math chars
\usepackage{eurosym} %% \euro symbol
\usepackage[T1]{fontenc} %% Vector fonts
\usepackage[babel]{microtype} % load microtype after babel, inputenc, and font-changing packages
% \usepackage{flushend}	% balanced last page in 2-col formatting
%
% Useful packages.  Optional.
%
\usepackage{graphicx}
\usepackage[numbers,square,sort&compress]{natbib}
\usepackage[defblank]{paralist} % inline lists
\usepackage{parskip} \parskip=1ex plus 5pt minus 1pt \parindent=4ex %% indent paragraphs
\usepackage[obeyspaces]{url} \urlstyle{sf} %% URLs 
% \usepackage[verbatim]{svn-multi}
\usepackage{xcolor}
\usepackage{xspace}
\usepackage{hyperref}  %% hyperlinks in PDF (must be last package loaded)
    \newcommand{\hhref}[2]{\href{#2}{#1}} %{text}{URL}

% \documentclass[11pt]{article}
% \usepackage{inputenc}
% \usepackage{amssymb}
% \usepackage{graphicx}
% \usepackage{xcolor}

\newcommand\todo[1]{\textcolor{blue}{(TODO: #1)}}
\newcommand{\commentaire}[2][fromWhom?]{{
  \color{magenta}{\bfseries\sffamily\scriptsize$\triangleright$(#1:) #2$\triangleleft$}
}}


\begin{document}
\author{Saalik Hatia}
\title{Specification of persistent storage for a TCC database}
\date\today
\maketitle

\begin{abstract}

  % Large-scale application are typically built on top of geo-distributed
  % databases running on multiple datacenters (DCs) situated around the
  % globe.
  % Network failures are unavoidable, but in most internet services,
  % availability is required; however, the CAP theorem proves
  % that it is impossible to provide both availability and strong
  % consistency at the same time.
  % Sacrificing strong consistency, exposes developpers to complex anomalies
  % that are hard to build against.
  
  % Checkpointing is a technique that is used in distributed 
  % systems to help tolerate failures and speed up recovery for years.
  % Distributed systems uses geo-replication to provide high availability and achieve 
  % low latency for the end user.
  % In each DC there is one journal per shard containing local updates, 
  % each of these updates are sent to its counterpart in other DCs.
  % Upon reception those updates remote DCs log them into a journal that 
  % contains updates received from that location.
  % Considering this at a single DC the number of journals amounts to the
  % number of shards multiplied by the number of datacenters i.e. a replica.
  
  % These journals grow without bound and in order to avoid storage issues 
  % we need to truncate them.
  % Before being able to truncate the journals safely, checkpoints need to be 
  % created and stored durably.

  AntidoteDB is database where data is persisted in the form of 
  a journal of operations. 
  As time goes this journal grows without bound impacting storage, access time and 
  recovery time.
  To address these shortcomings we propose a mechanism to prune the journal by 
  checkpointing essential information.
  Checkpoints should be consistent, made from stable updates at a point
  in time where concurrency information is no longer essential.
  % However AntidoteDB's current design makes checkpointing and pruning a challenge.
  
  Construction of an object's version in AntidoteDB, is done by reading all the updates made on the object.
  This method allows the implementation of multiversion concurrency
  control (MVCC) which enables multiple version of the same object to exist
  concurrently by reading the relevant operations.
  The consequence of building objects this way, is that each and every operation 
  have to be conserved forever.

  AntidoteDB replicates data between different datacenters (DC). 
  Within a DC, data is sharded between several nodes called shards.
  Each shard handles a portion of the keyspace and maintain its own
  journal of operations. 
  Each DC is a full replica of the database and has an identical number of 
  shards where data can be updated locally. 
  These updates are totally ordered within each DC.
  When data is propagated from one datacenter to another it creates a 
  partial order between updates from different locations.

  The combination of multiple journals independently updated at different 
  locations, MVCC and partial ordering between DCs makes checkpointing and 
  truncation a challenge.

  The main objective of this work is to specify a mechanism for
  pruning the journal safely, by storing consistent checkpoints.

  To properly choose a checkpointing cut, a set of point, one per log, 
  that represent a state of data at this point in time.
  We need to keep track of information throughout multiple shards 
  and further, multiple DCs.
  For every update there are three things to keep track of,
  causal dependencies in local shards, others coming from 
  remote DCs and finally updates that are part of the same
  transaction.
  If an update is part of a cut all of its dependencies need to be part of the 
  cut.

  First, we keep track of local operations in the database
  by tracking transactions, that are committed or in-flight.
  % Persistent operations, the committed and in-flight transactions.
  Then keep track of communication between shards in a DC, 
  and between DCs in order to track causal dependencies. 
  Using all this information we create intermediate conssitent cuts to 
  arrive at a point where: (1)updates are safe, (1) updates are stable,
  (3) there are no in-flight transactions, meaning concurrency information
  is no longer essential enabling the creation of a sequentially consistent
  snapshot.
  This sequential snapshot is then stored in a datastore called Checkpoint Store.

  % them it allow us to create consistent cuts 
  % that represent the state of the database through them.
  % One of them represent the point of safety in a datacenter called 
  % DC-wide causal safe point (DCSf) which ensures that all the updates 
  
  In our design the database is composed of three main components, Cache, 
  Journal and Checkpoint Store.
  The Journal is the logical representation of all the journals in each shard.
  The Checkpoint Store contains the checkpoint, we assume it is a sequentially
  consistent store.
  Finally the Cache is responsible for materializing object states from the 
  Journal.
  To ensure correctness of interactions between these components
  we will identify the invariants of every interaction, 
  the global system invariants using the maintained cuts. 

  The final step will be by writing the corresponding pseudo-code 
  and formally verify it. 
  And finally implement and validate through experimentation.
  
  
  % AntidoteDB is a database designed for geo-replication.
  % It aims to provide high availability with the strongest possible
  % consistency model, it guarantees Transactional Causal Consistency (TCC)
  % and supports CRDTs.
  % TCC means that: (1) if one update happens before another, they will be
  % observed in the same order (causal consistency), and (2) updates in the
  % same transaction are observed in all-or-nothing fashion.
  
  % In AntidoteDB, the database is persisted as a journal of operations.
  % In the current implementation, the journal grows without bound.
  % The main objective of this work is to specify a mechanism for
  % pruning the journal safely, by storing recent checkpoints.
  
  % Data is sharded, each shard is responsible for a portion of the 
  % keyspace.
  % The way transactions are handled by the database is by contacting
  % each shard concerned by the operations in them.
  % Each shard in a DC is replicated to all other DCs. 
  % All updates that originate in some DC are sent asynchronously to the
  % corresponding shards in other DCs.
  % Making it a local update journal and replicated journals from every
  % miror shard in other DCs.

  % In order to properly choose a checkpointing cut we need to keep track
  % of the state throughout multiple shards and further, multiple DCs.
  % Because an update can have dependencies in other shards journal.

  % We use consistent cuts to represent states of durability and all the 
  % running and committed transactions. 
  % We build upon them to find a safe cut called DCSf that represent the 
  % safety point from which transaction can read from.  
  
  % A secondary objective is to be able to use multiple storage backends,
  % including legacy databases or file systems without change to their
  % native format.

  

  % To enable pruning we need to keep track of the overall state of the database.
  % With this in mind we study the design and properties of the current
  % architecture.
  % Using information at our disposal we maintain states to allow us
  % to keep track of the local state of the datacenter first.
  % Using communication between datacenters we are able to infer the 
  % global state of our system. 
  % Once we have these states we use them to characterise the operations and 
  % correctness of the system using invariants.

  

% \url{https://saalik.github.io/sharedfiles/specification.pdf}

\end{abstract}
\end{document}
